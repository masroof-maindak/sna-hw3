#let title = [
  SNA Assignment \#03
]

#set page(
  paper: "us-letter",
  numbering: "1",
)

#show heading.where(
  level: 1
): it => block(width: 100%)[
  #set text(13pt)
  #strong(it.body)
]

#show heading.where(
  level: 2
): it => block(width: 100%)[
  #set align(center)
  #set text(13pt)
  #underline[#smallcaps(it.body)]
]


#align(center, text(16pt)[
  *#title*
])

#align(center)[
  Mujtaba Asim \
  #link("mailto:bscs22012@itu.edu.pk")
]


= Q1: If we repeatedly adjust $accent(w, arrow)$ w.r.t $accent(x, arrow)$ (misclassified), how many times may we need to adjust the former such that the latter gets classified correctly?

== Lines, Planes, Hyperplanes, $x$ and $w$

To begin with, I will explain some fundamental, underlying principles I had to
read up on/refresh my memory of in order to attempt this assignment.

The equation of a line is $a x + b y + c$ and the equation of a plane is $a x +
b y + c z + d$.

One could imagine this pattern extending onwards to infinity could trivially, but
it would quickly get irksome to write out, so instead we generalise the aforementioned
expressions like so: $w^T x = w_1 x_1 + w_2 x_2 + ... w_d x_d$ where $d$ is the
number of dimensions that $x$ is spread out across.

Each 'feature component' i.e 'part' of $x$ from an arbitrarily selected dimension
corresponds to the $x$, $y$, or $z$ i.e axes from the original equation (of
course, these are unwieldy to continue with once $d$ surpasses 3) and each part
of $w$, i.e $w_i$ is the coefficient of $x_i$ in that dimension.

All the weights together are referred to as the weight vector $w$. To take the
dot product of them with all the feature components of $x$, we must first trans
pose this vector so they form a row rather than a column vector.

Now, let us attempt to understand why we are even modifying $w$ in the first place.

== Orthogonality b/w $w$ and $w^T x$

To begin with, we must acknowledge that $w$ is perpendicular to the hyperplane
generated by $w^T x$. This is trivially provable by momentarily setting $d$ to 2.

$ w_1 x_1 + w_2 x_2 = 0 $
$ w^T x = 0 $

Pick any point $p$ and $q$ on the line.
$ w^T p = 0, w^T q = 0 $

One vector along the line is $q - p$. Thus:

$ w^T (q - p) = 0 $

And we know that the only time a dot product results in 0 is when the two input
vectors are orthogonal i.e perpendicular to each other. This same concept can be
extrapolated to higher dimensions.

As for why this orthogonality is important? We'll come back to that later.

== How is $w$ used for classification?

In the hyperplane itself, all the vectors $x$ such that $w^T x = 0$ lie exactly
on the plane itself. The act of classification is trivially 'segregating' a point
based on which side of the hyperplane it's fallen on.

If $w^T x > 0$, then the point $x$ is the on the same side of the hyperplane as
the weight vector $w$. If the dot product is a negative value, then the vector
$x$ must lie on the opposite side as that of $w$. The reason for this being the
$cos theta$ component in the dot product formula -- if $x$ is on the same side
as $w$, $theta < 90$, i.e $cos theta > 0$.

== How do we correct $w$?

Well, the first step is to understand _how far off_ from the hyperplane the vector
$x$ in question is.

To this end, the magnitude of the unsigned hyperplane expression, $|w^T x|$, tells
us the distance of the vector $x$ is from the hyperplane, perpendicularly (after
dividing it by the magnitude of $w$ i.e $||w||$).

=== And why must we divide by $||w||$ ?

The shortest distance from a point to a plane is always perpendicular to it.
$w$ is exactly this. Therefore, the distance from $x$ to the hyperplane boils
down to the question "how much of $x$ lies in the direction of $w$?" Pedantically,
this is the _projection length_ of $x$ onto $w$.

This can also be derived via some trigonometry rules, but for brevity's sake, it's
$||x|| cos theta$. Observing closely, we find it to be a subset of the dot product
expression generated for $w^T x$, and as such

$ ( ||w|| ||x|| cos theta ) / ( ||w|| ) = ||x|| cos theta $

=== Updation Procedure

Once the aforementioned is clear, the actual updation is quite straightforward.

$ w = w + y x $

Where $y$ is the true label and it's value is equal to either +1 or -1.
In the case of the former, $w$ moves towards $x$, thereby increasing the
prior-mentioned desirable 'projection of $x$ along $w$.' Conversely, in the case
of the latter, the expression becomes $w = w - x$, thereby taking $w$ *away* from $x$.

With all this background out of the way, we can now _finally_ start determining
how many $w$ updates are needed to correctly classify an arbitrary vector $x$

== Maximum Number of $w$ iterations

TODO.

= Q2
